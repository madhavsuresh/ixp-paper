\documentclass{acm_proc_article-sp}
\usepackage{hyperref}

\begin{document}
\setcounter{secnumdepth}{5}

\title{Determining Remote Peerings at IXPs}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{2} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Mario Sanchez\\
       \affaddr{Northwestern University}\\
       \affaddr{EECS Department}\\
       \affaddr{Evanston, IL}\\
       \email{msanchez@northwestern.edu}
% 2nd. author
\alignauthor
Madhav Suresh\\
       \affaddr{Northwestern University}\\
       \affaddr{EECS Department}\\
       \affaddr{Evanston, IL}\\
       \email{madhav@u.northwestern.edu}
}
\date{16 March 2012}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
Peering matrices of Internet exchange points (IXPs) have been an area of interest because of their
critical role in the flow of traffic. \cite{Augustin:2009}
Remote peering at IXPs has been a recent trend, as it allows smaller ASs to bypass more costly 
peering agreements.
Efforts have been made to discover peering matrices at IXPs however no work has been done 
to discover remote peers. We designed and implemented a tool that can determine remote peerings
given a peering list of an IXP. Using traceroutes obtained from Dasu \cite{Sanchez:2011}, geolocation
techniques and reverse dns, we were able to determine remote peerings with 
%TODO: 
around 50\% level of accuracy.
\end{abstract}

% A category with the (minimum) three required fields
%\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

%\terms{IXP, remote peering}

%\keywords{ACM proceedings, \LaTeX, text tagging} % NOT required for Proceedings

\section{Introduction}

Internet eXchange Points (IXPs) are critical to the fabric of the internet. They provide a place 
for smaller ASes to peer without having to pay through their noses to tier one providers. Work has
    been done to identify the peering matrices at these IXPs \cite{Augustin:2009}, however no work has 
    gone into finding remote peerings.
Remote peering at IXPs is the practice of peering without a collocated router at an IXP. Smaller ASs
who want to bypass transit links can use remote peering as a method to lower cost and meet geographic 
diversity requirements. Currently mapping tools exists for determining peering matrices at 
IXPs however nothing exists to determine which peers are remote peers.

This paper presents the design, implementation, and evaluation of a programmatic remote peer finder.



\section{System Design}
We implemented a multifiltering mechanism to avoid false negatives and increase confidence in our results.
First we determine which peerings we suspect to be those of long-haul links, 
then we verify that finding with geolocation. For added confidence,
we augment the mappings with reverse DNS names. 

\subsection{System Architecture}
\subsubsection{Determining Long Haul Links}
In a traditional IXP setup, all peers are present at the physical location of the IXP. 
As such every peer has a physical interface at router in the IXP, and the latency hop between
to peers comes directly from the latency of the router. Remote peers however are not physically 
present at the IXP. We found that many remote peers were connected through an MPLS tunnel.
MPLS is a layer two mechanism which bypasses traditional layer three routing, and sends data directly
from source to destination. Since MPLS is a layer two protocol, traditional traceroutes fail to capture when 
they traverse MPLS links. Unfortunately our dataset from Dasu only included traditional traceroutes. Paris traceroutes
are able to determine MPLS links, and we were able to run tests on LookingGlass nodes that offered them. This had to be done
manually so it was not practical. In the future we hope to add paris traceroutes to Dasu to facilitate this. 
In addition not all remote peers use MPLS links, so another method was also necessary to avoid a large number of
false negatives.  
We made the assumption that a peer that experiences a larger than average
latency hop is most likely a remote peer. There were of course false positives, but 
through our filtering process we hoped to remove of those. With a set of traceroutes from
Dasu \cite{Sanchez:2011} we narrowed down the traces to routes that were known to go through IXPs.
\subsubsection{Geolocation}
Given the IPs that we suspect to be longhaul links we use geolocation on the IP to determine where
the interface is located. If the interface is not located in the same area, the we have found a 
remote peering, if it is we discard the IP. To do the geolocation we used the Tulip tool from the Stanford
National Accerleator Lab. 
Tulip provides for four geolocation techniques: Apollonius,Trilateration, Geoip, and constraint based geolocation (CBG).
\paragraph{Trilateration}
Trilateration uses three vantage points in order to pin point the desired host. The estimation technique tries to find the intersection of 
circles with the radius determined by the latency between the two hosts. Problems can arise with this technique because of routing delays
which can lead to latencies that don't necessarily correspond with absolute distance. For example, a latency measurement can be slowed down by queueing delay
making the distance seem much longer than ground truth to the algorithm. Multilateration is a technique that attempts to overcome this through using more than three
vantage points (three is simply a minimum requirement), 
\paragraph{Constraint based Geolocation}
CBG is very similar to trilateration - it uses multilateration, however it attempts to transform delay measurements 
to geographic distance constraints and then applies multilateration to infer the real location. \cite{Gueye:2006}
A more in depth discussion can be found in Gueye et al. \cite{Gueye:2006}
\paragraph{Apollonius}
See \href{http://en.wikipedia.org/wiki/Problem\_of\_Apollonius}{Problem of Apollonius} for an in depth discussion.
\paragraph{GeoIP}
GeoIP has an enormous database of IP's and geolocation and does a simply key search. This can obviously lead
to great inaccuracy because of stale data, and even incorrect data


\subsubsection{Reverse DNS Geolocation}
Reverse DNS can be extremely powerful because DNS information can be used to get the general locale of a host. \cite{Spring:2004}
Reverse DNS geolocation works by looking at the hostname and matching it against a library of prefixes to determine an estimate as to the 
location. This works because ASs generally have a substring in the domain name, e.g. comcast hosts are ***.***.comcast.net
The authors of tulip noted that knowing the different geolocation techniques perform differently based on location
and provisioning.\cite{TulipInfoComm} 
For future work, armed with the locale of a host, we can elimiate certain geolocation techniques that we know will hurt the average.



\section{Implementation}
\subsection{Long Haul Latencies}
We implemented a java tool that formats traceroute latencies, and determines if the hop latency is decreasing monotonically.
A monotonic increase in latency was imperative because that implies that each hop is a greater distance from the host than the previous host.
In order to determine that the link is long haul, we look at the hop at the IXP or the hop after the IXP to the exiting AS.
A long haul link was characterized as having a hop that had latency greater than the average latency plus two times the standard deviation, where the 
standard deviation was greater than fifty.
$$ longhaul = rtt > mean_{rtt} + 2\cdot \sigma, \sigma > 50$$
We choose two as the multiplier value because it seemed like a good idea at the time. Given the longhaul links, we identified the ASes located adjacaent to the IXP hop and fed them into our 
other heuristics.
\subsection{Geolocation}
Given the long haul links, we had a python script which queried Tulip, and then fed the coordinates into the Google maps api in order to find the country code.
There were some issues with this because Tulip is currently under development. This lead to some of the metrics being intermittentenly unavailable.
\subsection{Reverse DNS}
For the IPs of suspected remote ASes we ran a reverse DNS query and tried to feed them to undns from scriptroute.
Unforunately the libraries that scriptroute uses are stale, which lead to a low match rate rendering this heuristic unusable.
An interesting note is that 771 of the hosts were pingable from their IPs but not pingable from their reverse DNS names.


After these heuristics we would either label suspected ASes as remote peers if they hit our criteria. Over all the traceroutes if ASes were consistently marked as remote peers then we globally deemed them remote peers. Over all the traceroutes if ASes were consistently marked as remote peers then we globally deemed them remote peers.


\section{Evaluation}
\label{sec:evaluation}
We evaluated our tool against using data available from AMS-IX, an IXP from Amersterdam, as ground truth.
AMS-IX has 483 member ASes with 410 directly connected, and 73 remotely connected.
We were able to identify around 8,000 peerings, 100 of which involved remote ASes.
\subsection{Accuracy of Tool}
Our tool was able to correctly around fifty percent of the remote peerings.
\{FILL OUT WITH ACTUAL DATA\}
\section{Discussion}
We were faced with a couple of challenges that affected the confidence of our results.
\subsection{Long Haul Link Issues}
Our tool came up with an extremely high number of false positives for remote peerings. As first step, we can eliminate this
through using more traceroutes through the ASes and only taking an average of the latencies. While we did only identify peers as 
remote if they consistently were categorized as long haul, we didnt take averages from specific routes. Doing this could help eliminate 
outliers due to congestion and other network issues. Something we found was that peers not defined as ``remote'' were using long haul links
to connect to the IXP. This underlines the fact that remote peers has a purely economic meaning. We are still looking for a way to battle this,
because right now our tool simply finds long haul links. In addition the use of MPLS confuses traceroutes as sometimes MPLS routers don't 
return the outgoing interface. This results in incomplete traceroutes, which greatly confuses our tool. For example, for a peer that was confirmed to be 
a normal peer at the IXP, the traceroute indicated that a hop went from the incoming AS, through the IXP, and then to the AS in another country. This was not corroborated by the ground truth, so we explained it as being
an artifact of MPLS. When running paris traceroutes on our machines, we weren't able to get
MPLS labels - only going through LookingGlass nodes seemed to work. 

What we need is another variable to add other than long latency jumps, since these seem to be prevalent in non remote peers.
\subsection{Geolocation}
Currently Tulip is under development, so it changed as we used it over time. Tulip provides an excellent set of data points, but discussion with the creators of the tool indicated that there are some bugs that 
are still prevalent. This reduces our confidence in the geolocation, and may help explain the erratic nature of our results. In the future it would be interesting to implement a geolocation 
technique on Dasu. Currently Dasu is unaware of the location of each of the nodes, but perhaps by adding a field for users to enter their location, or even performing CBG on the nodes to determine their location might help.
A setup where nodes were added as vantage points for geolocation as soon as their location was determined could be interesting. Of course the geographic diversity of Dasu could prove to be a problem, but hopefully it would
be better than what existing tools provide. In addition some Dasu nodes might not be in well provisioned networks which could cause issues with being a vantage point in a geolocation scheme. 

In addition getting a relevant library for the reverse dns geolocation could be useful as it would help
us narrow down which method to use and which vantage points to select for geolocation.
What we learned is that our tool needs to be more intelligent than what it currently is. Our approach is just one heuristic applied after another, however a more intelligent design would
choose how to apply a heuristic based on the previous result. This could prove to be more powerful and hopefully reduce the noise in our results.

\section{Future Work}
\label{sec:future}
Much to do.

\bibliographystyle{abbrv}
\bibliography{ixp}

\end{document}
