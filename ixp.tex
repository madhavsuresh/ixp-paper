\documentclass{acm_proc_article-sp}

\begin{document}
\setcounter{secnumdepth}{5}

\title{Determining Remote Peerings at IXPs}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{2} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Mario Sanchez\\
       \affaddr{Northwestern University}\\
       \affaddr{EECS Department}\\
       \affaddr{Evanston, IL}\\
       \email{msanchez@northwestern.edu}
% 2nd. author
\alignauthor
Madhav Suresh\\
       \affaddr{Northwestern University}\\
       \affaddr{EECS Department}\\
       \affaddr{Evanston, IL}\\
       \email{madhav@u.northwestern.edu}
}
\date{16 March 2012}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
Peering matrices of Internet exchange points (IXPs) have been an area of interest because of their
critical role in the flow of traffic. \cite{Augustin:2009}
Remote peering at IXPs has been a recent trend, as it allows smaller ASs to bypass more costly 
peering agreements.
Efforts have been made to discover peering matrices at IXPs however no work has been done 
to discover remote peers. We designed and implemented a tool that can determine remote peerings
given a peering list of an IXP. Using traceroutes obtained from Dasu \cite{Sanchez:2011}, geolocation
techniques and reverse dns, we were able to determine remote peerings with 
%TODO: 
(WHAT?) level of accuracy.
\end{abstract}

% A category with the (minimum) three required fields
%\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

%\terms{IXP, remote peering}

%\keywords{ACM proceedings, \LaTeX, text tagging} % NOT required for Proceedings

\section{Introduction}

\{include stuff about IXPs and whatnot here\}

Remote peering at IXPs is the practice of peering without a collocated router at an IXP. Smaller ASs
who want to bypass transit links can use remote peering as a method to lower cost and meet geographic 
diversity requirements. Currently mapping tools exists for determining peering matrices at 
IXPs however nothing exists to determine which peers are remote peers.

This paper presents the design, implementation, and evaluation of a programmatic remote peer finder
that is capable of determining remote peers with a (WHAT?) level of confidence. 



\section{System Design}
We implemented a multifiltering mechanism to avoid false negatives and increase confidence in our results.
First we determine which peerings we suspect to be those of long-haul links, 
then we verify that finding with geolocation. For added confidence,
we augment the mappings with reverse DNS names. 

\subsection{System Architecture}
\subsubsection{Determining Long Haul Links}
In a traditional IXP setup, all peers are present at the physical location of the IXP. 
As such every peer has a physical interface at router in the IXP, and the latency hop between
to peers comes directly from the latency of the router. Remote peers however are not physically 
present at the IXP. We found that many remote peers were connected through an MPLS tunnel.
MPLS is a layer two mechanism which bypasses traditional layer three routing, and sends data directly
from source to destination. Since MPLS is a layer two protocol, traditional traceroutes fail to capture when 
they traverse MPLS links. Unfortunately our dataset from Dasu only included traditional traceroutes. Paris traceroutes
are able to determine MPLS links, and we were able to run tests on LookingGlass nodes that offered them. This had to be done
manually so it was not practical. In the future we hope to add paris traceroutes to Dasu to facilitate this. 
In addition not all remote peers use MPLS links, so another method was also necessary to avoid a large number of
false negatives.  
We made the assumption that a peer that experiences a larger than average
latency hop is most likely a remote peer. There were of course false positives, but 
through our filtering process we hoped to remove of those. With a set of traceroutes from
Dasu \cite{Sanchez:2011} we narrowed down the traces to routes that were known to go through IXPs.
We then set a threshold for latency around \{WHAT WAS THE LATENCY?\}ms which we determined through
looking at the average trace of peers connected at the IXP. At this point the system has ouputted 
what it thinks are long haul links. It then moves to corroborate this set against geolocation data.
\subsubsection{Geolocation}
Given the IPs that we suspect to be longhaul links we use geolocation on the IP to determine where
the interface is located. If the interface is not located in the same area, the we have found a 
remote peering, if it is we discard the IP. To do the geolocation we used the tulip tool from the Stanford
National Accerleator Lab. One of the problems we face with geolocation was the 
accuracy from the various geolocation techniques. Tulip provides for four geolocation techniques: Apollonius,Trilateration, Geoip, and constraint based geolocation (CBG).
\paragraph{Trilateration}
Trilateration uses three vantage points in order to pin point the desired host. The estimation technique tries to find the intersection of 
circles with the radius determined by the latency between the two hosts. Problems can arise with this technique because of routing delays
which can lead to latencies that don't necessarily correspond with absolute distance. For example, a latency measurement can be slowed down by queueing delay
making the distance seem much longer than ground truth to the algorithm. Multilateration is a technique that attempts to overcome this through using more than three
vantage points (three is simply a minimum requirement), 
\paragraph{Constraint based Geolocation}
CBG is very similar to trilateration - it uses multilateration, however it attempts to transform delay measurements 
to geographic distance constraints and then applies multilateration to infer the real location. \cite{Gueye:2006}
A more in depth discussion can be found in Gueye et al. \cite{Gueye:2006}
\paragraph{Apollonius}
\paragraph{GeoIP}
GeoIP has an enormous database of IP's and geolocation and does a simply key search. This can obviously lead
to great inaccuracy because of stale data, and even incorrect data


\subsubsection{Reverse DNS Geolocation}
Reverse DNS can be extremely powerful because DNS information can be used to get the general locale of a host. \cite{Spring:2004}
Reverse DNS geolocation works by looking at the hostname and matching it against a library of prefixes to determine an estimate as to the 
location. This works because ASs generally have a prefix \{MARIO IS THIS THE RIGHT WORD\} e.g. comcast hosts are ***.***.comcast.net
The authors of tulip noted that knowing the different geolocation techniques perform differently based on location
and provisioning.\cite{TulipInfoComm} 
For future work, armed with the locale of a host, we can elimiate certain geolocation techniques that we know will hurt the average.



\section{Implementation}
\subsection{Long Haul Latencies}
We implemented a java tool that formats traceroute latencies, and determines if the hop latency is decreasing monotonically.
A monotonic increase in latency was imperative because that implies that each hop is a greater distance from the host than the previous host.
In order to determine that the link is long haul, we look at the hop at the IXP or the hop after the IXP to the exiting AS.
A long haul link was characterized as having a hop that had latency greater than the average latency plus two times the standard deviation, where the 
standard deviation was greater than fifty.
$$ longhaul = rtt > mean_{rtt} + 2\cdot \sigma, \sigma > 50$$
We choose two as the multiplier value because it seemed like a good idea at the time. Given the longhaul links, we identified the ASes located adjacaent to the IXP hop and fed them into our 
other heuristics.
\subsection{Geolocation}
Given the long haul links, we had a python script which queried Tulip, and then fed the coordinates into the Google maps api in order to find the country code.
There were some issues with this because Tulip is currently under development. This lead to some of the metrics being intermittentenly unavailable.

Our protoype implements the first two stages that we outlined. We were unable to complete the reverse dns step, as existing tools 
had stale libraries that didn't match many of the hosts we had. For the long haul links we wrote a java program that analyzed traceroutes
from Dasu and for the geolocation. We used tulip as well as google maps api to determine country location in a python script.
A location esimate was accepted only if the country was the same for two out of the three techniques.
 As mentioned in the previous discussion for future work we plan on 
narrowing down which of the geolocation techniques are valid for a given region first in order to get more results.

\section{Evaluation}
\label{sec:approach}

\section{Future Work}
\label{sec:future}
Much to do.

\bibliographystyle{abbrv}
\bibliography{ixp}

\end{document}
